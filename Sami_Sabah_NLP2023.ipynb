{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QqJbxx-_BU7"
      },
      "source": [
        "#Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fy54k2O-Q3N"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch torchvision\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install datasets\n",
        "!pip install pytorch_transformers\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J13iSqp__qMw"
      },
      "source": [
        "Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-shbksg4Wz2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import (TensorDataset, DataLoader,\n",
        "                              RandomSampler, SequentialSampler)\n",
        "\n",
        "from pytorch_transformers import BertTokenizer, BertConfig\n",
        "from pytorch_transformers import BertForSequenceClassification\n",
        "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
        "\n",
        "from distutils.version import LooseVersion as LV\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
        "import torch.nn.functional as F\n",
        "import io\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "#matplotlib.use('TkAgg')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "sns.set()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    devicename = '['+torch.cuda.get_device_name(0)+']'\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    devicename = \"\"\n",
        "    \n",
        "print('Using PyTorch version:', torch.__version__,\n",
        "      'Device:', device, devicename)\n",
        "assert(LV(torch.__version__) >= LV(\"1.0.0\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXhFhTFsYSGM"
      },
      "source": [
        "Download the IMDb and SST-2 datasets and extract them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2w3DXq8zmL7"
      },
      "outputs": [],
      "source": [
        "# Load the IMDB dataset\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "\n",
        "# Load the SST-2 dataset\n",
        "sst2_dataset = load_dataset(\"glue\", \"sst2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw5Lk4nFXf1A"
      },
      "source": [
        "Load the IMDb dataset using pandas, and preprocess the text data by removing HTML tags, non-alphanumeric characters, and stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoEDXTDQr89F",
        "outputId": "03fcbdf6-a82c-4e52-ca0d-a8924b2c63f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': ['text', 'label'], 'test': ['text', 'label'], 'unsupervised': ['text', 'label']}\n"
          ]
        }
      ],
      "source": [
        "print(imdb_dataset.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "dXioOoLXj5_6",
        "outputId": "d6b9cb6e-a0d6-40c9-c88d-1f27967ff6c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IMDB data loaded:\n",
            "data set: (50000, 2)\n",
            "[0 1]\n"
          ]
        }
      ],
      "source": [
        "# Load the IMDb dataset\n",
        "imdb_df = pd.concat([pd.DataFrame(imdb_dataset['train']),pd.DataFrame(imdb_dataset['test'])])\n",
        "imdb_df = imdb_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print('\\nIMDB data loaded:')\n",
        "print('data set:', imdb_df.shape)\n",
        "print(imdb_df['label'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5ajmdUSTPKj"
      },
      "source": [
        "Load the SST-2 dataset using pandas, and preprocess the text data in the same way as the IMDb dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPKsg4shr4zn",
        "outputId": "cd5a91b8-dc22-4223-dc69-4f33c9815e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': ['sentence', 'label', 'idx'], 'validation': ['sentence', 'label', 'idx'], 'test': ['sentence', 'label', 'idx']}\n"
          ]
        }
      ],
      "source": [
        "print(sst2_dataset.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sHU-_OllRtQ",
        "outputId": "a54898a8-f2ef-4e2b-b004-e408dcdb7ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SST2 data loaded:\n",
            "data set: (68221, 2)\n",
            "[0 1]\n"
          ]
        }
      ],
      "source": [
        "# Load the SST-2 dataset\n",
        "\n",
        "sst2_df = pd.concat([pd.DataFrame(sst2_dataset['train'])[['sentence', 'label']],pd.DataFrame(sst2_dataset['validation'])[['sentence', 'label']]])\n",
        "sst2_df = sst2_df.rename(columns={'sentence': 'text'})\n",
        "sst2_df = sst2_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print('\\nSST2 data loaded:')\n",
        "print('data set:', sst2_df.shape)\n",
        "print(sst2_df['label'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EkwE9ffRA5YB"
      },
      "outputs": [],
      "source": [
        "# Preprocess the text data\n",
        "sst2_df ['text'] = sst2_df ['text'].str.replace('<.*?>', '', regex=True) # remove HTML tags\n",
        "sst2_df ['text'] = sst2_df ['text'].str.replace('[^a-zA-Z0-9\\s]', '', regex=True) # remove non-alphanumeric characters\n",
        "stop_words = set(stopwords.words('english'))\n",
        "sst2_df ['text'] = sst2_df ['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words])) # remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ2PQoK3BMxC",
        "outputId": "f2fcd84a-ba35-47fa-910f-18052b99221e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  label\n",
            "41637  rare family movie genuine sweet without relyin...      1\n",
            "28062                                    even reassuring      1\n",
            "12011                           subtlety never trademark      0\n",
            "41550  psychology real narrative logic series careful...      0\n",
            "48956  pop cinematic year already littered celluloid ...      0\n"
          ]
        }
      ],
      "source": [
        "# Let's view some random reviews:\n",
        "print(sst2_df.sample(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSS15bBzr0lN"
      },
      "source": [
        "Split into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trDNhh0nsDGL",
        "outputId": "10663e53-f7da-4198-eb93-26a1adcef00d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (54576, 1)\n",
            "X_test shape: (13645, 1)\n",
            "y_train shape: (54576,)\n",
            "y_test shape: (13645,)\n",
            "\n",
            "SST2 data re splitted:\n",
            "train: (54576, 2)\n",
            "test: (13645, 2)\n",
            "[1 0]\n",
            "[1 0]\n"
          ]
        }
      ],
      "source": [
        "# Define your features and target variable\n",
        "X = sst2_df.drop(\"label\", axis=1)\n",
        "y = sst2_df[\"label\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shape of the train and test sets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "sst2_train_df = pd.concat([X_train,y_train], axis=1)\n",
        "sst2_test_df = pd.concat([X_test,y_test], axis=1)\n",
        "\n",
        "print('\\nSST2 data re splitted:')\n",
        "print('train:', sst2_train_df.shape)\n",
        "print('test:', sst2_test_df.shape)\n",
        "print(sst2_train_df['label'].unique())\n",
        "print(sst2_test_df['label'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnTy7rj631jm",
        "outputId": "f850eacd-0c8c-4146-aa4e-d0f6ec4bdb53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 text  label\n",
            "55980  filmmakers want nothing else show us good time      1\n",
            "58818                           one alternate reality      0\n",
            "34216                                  nt funny hoped      0\n",
            "64863                       nt even bother rent video      0\n",
            "35549                                      disturbing      0\n",
            "                                                    text  label\n",
            "27840                           supposed romantic comedy      0\n",
            "41442  300 hundred years russian cultural identity st...      1\n",
            "49660                           enervating determination      1\n",
            "54246  secretary takes unexpected material handles un...      1\n",
            "32648                                               back      0\n"
          ]
        }
      ],
      "source": [
        "# Let's view some random reviews:\n",
        "print(sst2_train_df.sample(5))\n",
        "print(sst2_test_df.sample(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnVplK7CuFem"
      },
      "source": [
        "IN-DS: SST2\n",
        "OOD-DS: IMDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "stJtXnEyuE5h"
      },
      "outputs": [],
      "source": [
        "#Temporary limit the IN-DS size to 15%\n",
        "n = 0.15\n",
        "\n",
        "train_df = sst2_train_df.sample(int(n*sst2_train_df.shape[0]))\n",
        "test_df = sst2_test_df.sample(int(n*sst2_test_df.shape[0]))\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "n_ood = 0.3\n",
        "ood_df = imdb_df.sample(int(n_ood*train_df.shape[0]))\n",
        "ood_df = ood_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbFj52jjon7X",
        "outputId": "a249b16e-8d5c-40f5-897e-555138e5f0fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text  label\n",
            "2204  I saw kung fu movie I kid I thought cool Now I...      1\n",
            "363   In film I prefer Deacon Frost Hes sexy I love ...      1\n",
            "1668  There one film I think might good better one c...      1\n",
            "1144  Not best Lone Star series moves along quickly ...      0\n",
            "2100  Relentlessly stupid nobudget war picture made ...      0\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the text data\n",
        "ood_df ['text'] = ood_df ['text'].str.replace('<.*?>', '', regex=True) # remove HTML tags\n",
        "ood_df ['text'] = ood_df ['text'].str.replace('[^a-zA-Z0-9\\s]', '', regex=True) # remove non-alphanumeric characters\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ood_df ['text'] = ood_df ['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words])) # remove stop words\n",
        "\n",
        "\n",
        "# Let's view some random reviews:\n",
        "print(ood_df.sample(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fd4YYYJ2-OF1"
      },
      "outputs": [],
      "source": [
        "del X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZuRWQHKTbVn"
      },
      "source": [
        "The token `[CLS]` is a special token required by BERT at the beginning of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPKOd23EMA5h",
        "outputId": "55e091df-c846-41b5-f208-020dd2161092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The first training sentence:\n",
            "[CLS] dumb gags anatomical humor character cliches LABEL: 0\n"
          ]
        }
      ],
      "source": [
        "sentences_train = train_df.text.values\n",
        "sentences_train = [\"[CLS] \" + s for s in sentences_train]\n",
        "\n",
        "sentences_test = test_df.text.values\n",
        "sentences_test = [\"[CLS] \" + s for s in sentences_test]\n",
        "\n",
        "sentences_ood = ood_df.text.values\n",
        "sentences_ood = [\"[CLS] \" + s for s in sentences_ood]\n",
        "\n",
        "\n",
        "labels_train = train_df.label.values\n",
        "labels_test  = test_df.label.values\n",
        "labels_ood  = ood_df.label.values\n",
        "\n",
        "print (\"\\nThe first training sentence:\")\n",
        "print(sentences_train[0], 'LABEL:', labels_train[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71yBR1tYTeFz"
      },
      "source": [
        "Next we use the BERT tokenizer to convert the sentences into tokens\n",
        "that match the data BERT was trained on.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pTzOWtWTe0w",
        "outputId": "dc3fb1b2-6a1c-4861-b725-0a0c931892d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/231508 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 1024/231508 [00:00<00:49, 4624.15B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 15%|█▌        | 34816/231508 [00:00<00:02, 90346.52B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 45%|████▌     | 104448/231508 [00:00<00:00, 187808.10B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 253488.13B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The full tokenized first training sentence:\n",
            "['[CLS]', 'dumb', 'gag', '##s', 'anatomical', 'humor', 'character', 'cl', '##iche', '##s']\n",
            "\n",
            "The full tokenized first test sentence:\n",
            "['[CLS]', 'blockbuster', '##s', 'poll', '##ute', 'summer', 'movie', 'pool']\n",
            "\n",
            "The full tokenized first OOD sentence:\n",
            "['[CLS]', 'spoil', '##ers', '##i', 'pleasantly', 'surprised', 'find', 'harsh', 'criticisms', 'acting', 'dated', 'dialogue', 'unclear', 'storyline', 'un', '##founded', 'bela', '##fo', '##nte', 'great', 'brand', '##oes', '##que', 'menacing', 'swearing', 'spirit', 'must', 'earn', 'wings', 'realistic', '##ally', 'ill', '##e', '##qui', '##pped', 'past', 'life', 'he', 'learns', 'late', 'empty', 'hu', '##st', '##ling', 'material', '##istic', 'life', 'without', 'love', 'most', '##el', 'likewise', 'great', 'anguish', '##ed', 'man', 'dying', 'wife', 'fanny', 'in', 'spite', 'prayers', 'miracle', 'bitterness', 'prevents', 'accepting', 'believing', 'one', 'the', 'two', 'social', 'worlds', 'characters', 'represent', 'alternately', 'col', '##lide', 'complement', 'result', 'hilarious', 'touching', '##ly', 'sad', '##the', 'per', '##plex', '##ing', 'ending', 'actually', 'quite', 'consistent', 'rest', 'film', 'after', 'looking', 'everywhere', 'bela', '##fo', '##nte', 'most', '##el', 'looks', 'see', 'falling', 'feather', 'frantically', 'reaches', 'he', '##s', 'finally', 'willing', 'believe', 'angels', 'miracles', 'but', 'bela', '##fo', '##nte', 'wasn', '##t', 'allowed', 'finish', 'miracle', 'either', 'restore', 'fanny', '##s', 'health', 'most', '##els', 'faith', 'never', 'got', 'wings', 'the', 'feather', 'floats', 'taunting', '##ly', 'most', '##els', 'grasp', 'metaphor', 'men', '##s', 'live', 'late', 'don', '##t', 'get', 'second', 'chance', 'like', 'its', 'wonderful', 'life', 'movie', 'magical', 'wonderful', 'funny', 'terribly', 'tragic']\n"
          ]
        }
      ],
      "source": [
        "BERTMODEL = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(BERTMODEL,\n",
        "                                          do_lower_case=True)\n",
        "\n",
        "tokenized_train = [tokenizer.tokenize(s) for s in sentences_train]\n",
        "tokenized_test  = [tokenizer.tokenize(s) for s in sentences_test]\n",
        "tokenized_ood  = [tokenizer.tokenize(s) for s in sentences_ood]\n",
        "\n",
        "print (\"\\nThe full tokenized first training sentence:\")\n",
        "print (tokenized_train[0])\n",
        "\n",
        "print (\"\\nThe full tokenized first test sentence:\")\n",
        "print (tokenized_test[0])\n",
        "\n",
        "print (\"\\nThe full tokenized first OOD sentence:\")\n",
        "print (tokenized_ood[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGEGsXZLThMZ"
      },
      "source": [
        "\n",
        "Now we set the maximum sequence lengths for our training and test\n",
        "sentences as `MAX_LEN_TRAIN` and `MAX_LEN_TEST`. The maximum length\n",
        "supported by the used BERT model is 512.\n",
        "\n",
        "The token `[SEP]` is another special token required by BERT at the\n",
        "end of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "YxtWll1wThtk",
        "outputId": "5c3572e5-ee61-47bb-a079-5bb9fef94e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The truncated tokenized first training sentence:\n",
            "['[CLS]', 'dumb', 'gag', '##s', 'anatomical', 'humor', 'character', 'cl', '##iche', '##s', 'SEP']\n"
          ]
        }
      ],
      "source": [
        "MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n",
        "\n",
        "tokenized_train = [t[:(MAX_LEN_TRAIN-1)]+['SEP'] for t in tokenized_train]\n",
        "tokenized_test  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_test]\n",
        "tokenized_ood  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_ood]\n",
        "\n",
        "print (\"\\nThe truncated tokenized first training sentence:\")\n",
        "print (tokenized_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FDmuiZ5VaOQ"
      },
      "source": [
        "\n",
        "Next we use the BERT tokenizer to convert each token into an integer\n",
        "index in the BERT vocabulary. We also pad any shorter sequences to\n",
        "`MAX_LEN_TRAIN` or `MAX_LEN_TEST` indices with trailing zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0LgugShVasR",
        "outputId": "5078ff88-204e-4d97-9877-75ec7c33a2a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The indices of the first training sentence:\n",
            "[  101 12873 18201  2015 28141  8562  2839 18856 17322  2015   100     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ],
      "source": [
        "ids_train = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_train]\n",
        "ids_train = np.array([np.pad(i, (0, MAX_LEN_TRAIN-len(i)),\n",
        "                             mode='constant') for i in ids_train])\n",
        "\n",
        "ids_test = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_test]\n",
        "ids_test = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)),\n",
        "                            mode='constant') for i in ids_test])\n",
        "\n",
        "\n",
        "ids_ood = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_ood]\n",
        "ids_ood = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)),\n",
        "                            mode='constant') for i in ids_ood])\n",
        "\n",
        "print (\"\\nThe indices of the first training sentence:\")\n",
        "print (ids_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXHXkvs5JTet"
      },
      "source": [
        "BERT also requires *attention masks*, with 1 for each real token in\n",
        "the sequences and 0 for the padding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "2yaiNP45Dkcm"
      },
      "outputs": [],
      "source": [
        "amasks_train, amasks_test , amasks_ood = [], [] , []\n",
        "\n",
        "for seq in ids_train:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  amasks_train.append(seq_mask)\n",
        "\n",
        "for seq in ids_test:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  amasks_test.append(seq_mask)\n",
        "\n",
        "\n",
        "for seq in ids_ood:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  amasks_ood.append(seq_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko7iCp5cJiyd"
      },
      "source": [
        "We use scikit-learn's train_test_split() to use 10% of our training\n",
        "data as a validation set, and then convert all data into\n",
        "torch.tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "p3djaDPDJnZ9"
      },
      "outputs": [],
      "source": [
        "(train_inputs, validation_inputs,\n",
        " train_labels, validation_labels) = train_test_split(ids_train, labels_train,\n",
        "                                                     random_state=42,\n",
        "                                                     test_size=0.1)\n",
        "(train_masks, validation_masks,\n",
        " _, _) = train_test_split(amasks_train, ids_train,\n",
        "                          random_state=42, test_size=0.1)\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_masks  = torch.tensor(train_masks)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks  = torch.tensor(validation_masks)\n",
        "test_inputs = torch.tensor(ids_test)\n",
        "test_labels = torch.tensor(labels_test)\n",
        "test_masks  = torch.tensor(amasks_test)\n",
        "ood_inputs = torch.tensor(ids_ood)\n",
        "ood_labels = torch.tensor(labels_ood)\n",
        "ood_masks  = torch.tensor(amasks_ood)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4TDX7VtJ2aA"
      },
      "source": [
        "Next we create PyTorch *DataLoader*s for all data sets.\n",
        "For fine-tuning BERT on a specific task, the authors recommend a\n",
        "batch size of 16 or 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AjgT4K4J2Fv",
        "outputId": "d9228039-cbf8-4fdd-ea46-33c08561a102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Datasets:\n",
            "Train: 7367 reviews\n",
            "Validation: 819 reviews\n",
            "Test: 2046 reviews\n",
            "OOD: 2455 reviews\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "print('\\nDatasets:')\n",
        "print('Train: ', end=\"\")\n",
        "train_data = TensorDataset(train_inputs, train_masks,\n",
        "                           train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler,\n",
        "                              batch_size=BATCH_SIZE)\n",
        "print(len(train_data), 'reviews')\n",
        "\n",
        "print('Validation: ', end=\"\")\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks,\n",
        "                                validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data,\n",
        "                                   sampler=validation_sampler,\n",
        "                                   batch_size=BATCH_SIZE)\n",
        "print(len(validation_data), 'reviews')\n",
        "\n",
        "print('Test: ', end=\"\")\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler,\n",
        "                             batch_size=BATCH_SIZE)\n",
        "print(len(test_data), 'reviews')\n",
        "\n",
        "\n",
        "print('OOD: ', end=\"\")\n",
        "ood_data = TensorDataset(ood_inputs, ood_masks, ood_labels)\n",
        "ood_sampler = SequentialSampler(ood_data)\n",
        "ood_dataloader = DataLoader(ood_data, sampler=ood_sampler,\n",
        "                             batch_size=BATCH_SIZE)\n",
        "print(len(ood_data), 'reviews')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4Wi5wQqKEuc"
      },
      "source": [
        "BERT MODEL INITIALIZATION\n",
        "\n",
        "We now load a pretrained BERT model with a single linear\n",
        "classification layer added on top.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdIeyBLOJ2DE",
        "outputId": "82f2b981-c5ab-43f4-b868-25a8c8724a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 433/433 [00:00<00:00, 135532.36B/s]\n",
            "100%|██████████| 440473133/440473133 [00:37<00:00, 11650771.84B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pretrained BERT model \"bert-base-uncased\" loaded\n"
          ]
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(BERTMODEL,\n",
        "                                                      num_labels=2,\n",
        "                                                      output_hidden_states=True)\n",
        "\n",
        "\n",
        "model.cuda()\n",
        "print('\\nPretrained BERT model \"{}\" loaded'.format(BERTMODEL))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynTgfHzeLUXY"
      },
      "source": [
        "\n",
        "We set the remaining hyperparameters needed for fine-tuning the\n",
        "pretrained model: \n",
        " * EPOCHS: the number of training epochs in fine-tuning\n",
        "   (recommended values between 2 and 4) \n",
        " * WEIGHT_DECAY: weight decay for the Adam optimizer \n",
        " * LR: learning rate for the Adam optimizer \n",
        "   (2e-5 to 5e-5 recommended) \n",
        " * WARMUP_STEPS: number of warmup steps to (linearly) reach the\n",
        "   set learning rate\n",
        "\n",
        " We also need to grab the training parameters from the pretrained\n",
        " model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZSPy9RhsJ2AT"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 4\n",
        "WEIGHT_DECAY = 0.01\n",
        "LR = 2e-5\n",
        "WARMUP_STEPS =int(0.2*len(train_dataloader))\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters()\n",
        "                if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay': WEIGHT_DECAY},\n",
        "    {'params': [p for n, p in model.named_parameters()\n",
        "                if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=1e-8)\n",
        "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=WARMUP_STEPS,\n",
        "                                 t_total=len(train_dataloader)*EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpTM0QR9Lyee"
      },
      "source": [
        "LEARNING\n",
        "\n",
        "Let's now define functions to train() and evaluate() the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "XBbxjNth2BoE"
      },
      "outputs": [],
      "source": [
        "def train(epoch, loss_vector=None, log_interval=200):\n",
        "    # Set model to training mode\n",
        "    model.train().to(device)\n",
        "\n",
        "    # Loop over each batch from the training set\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Copy data to GPU if needed\n",
        "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Zero gradient buffers\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            # Forward pass\n",
        "            loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,\n",
        "                         labels=b_labels)[0]\n",
        "\n",
        "        if loss_vector is not None:\n",
        "            loss_vector.append(loss.item())\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Clear unused variables\n",
        "        del  b_input_mask, b_labels\n",
        "\n",
        "        if step % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch, step * len(b_input_ids), len(train_dataloader.dataset),\n",
        "                  100. * step / len(train_dataloader), loss.item()))\n",
        "            \n",
        "    # Clear unused variables\n",
        "    del b_input_ids,batch, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6S9746L21fMw"
      },
      "outputs": [],
      "source": [
        "def evaluate(loader):\n",
        "  model.eval()\n",
        "\n",
        "  n_correct, n_all = 0, 0\n",
        "\n",
        "  for batch in loader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "      return outputs\n",
        "      logits = outputs[0]\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "\n",
        "    labels = b_labels.to('cpu').numpy()\n",
        "    n_correct += np.sum(predictions == labels)\n",
        "    n_all += len(labels)\n",
        "\n",
        "  print('Accuracy: [{}/{}] {:.4f}'.format(n_correct, n_all,\n",
        "                                          n_correct/n_all))\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMuuXi6lL65Q"
      },
      "source": [
        "Now we are ready to train our model using the train()\n",
        "function. After each epoch, we evaluate the model using the\n",
        "validation set and evaluate()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-TeGUafJ16j"
      },
      "outputs": [],
      "source": [
        "train_lossv = []\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print()\n",
        "    train(epoch, train_lossv)\n",
        "    print('\\nValidation set:')\n",
        "    evaluate(validation_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5EOhawoaTit"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Ce texte est au format code\n",
        "```\n",
        "\n",
        "# OOD detection "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y38GBSdMzNnq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "mVLLg9QQBFpv"
      },
      "outputs": [],
      "source": [
        "#define the function that calculates the metrics \n",
        "def metrics(scores: np.ndarray, labels: np.ndarray, threshold: float):\n",
        "    pos = np.where(scores >= threshold) \n",
        "    neg = np.where(scores < threshold)\n",
        "    n_pos = len(pos[0])\n",
        "    n_neg = len(neg[0])\n",
        "\n",
        "    tp = np.sum(labels[pos])\n",
        "    fp = n_pos - tp\n",
        "    fn = np.sum(labels[neg])\n",
        "    tn = n_neg - fn\n",
        "\n",
        "    FPR = fp / (fp + tn)\n",
        "    Accuracy = (tp+tn)/len(scores)\n",
        "    ERR = 1 - Accuracy\n",
        "    return FPR, ERR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "JCqf4lF4zOKR"
      },
      "outputs": [],
      "source": [
        "#the aggregation function where we define the number of layers used\n",
        "def aggregation(all_layers,num_layers):\n",
        "  agg_layers = all_layers[-1][:,0,:]\n",
        "  for i in range(-2,-num_layers,-1):\n",
        "    agg_layers += (1/num_layers)*all_layers[-i][:,0,:]\n",
        "  return agg_layers\n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ouof2Fpl7tWC"
      },
      "outputs": [],
      "source": [
        "def extract_bert_features(loader,num_layers):\n",
        "    model.eval()\n",
        "\n",
        "    label_list = []\n",
        "    pred_list = []\n",
        "    agg_layers_list = []\n",
        "    for batch in loader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "            agg_layers = aggregation(outputs[1],num_layers)\n",
        "            logits = outputs[0]\n",
        "            predictions = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "\n",
        "        agg_layers_list.append(agg_layers)\n",
        "        label_list.append(b_labels.cpu().numpy())\n",
        "        pred_list.append(predictions)\n",
        "\n",
        "    agg_layers_list = torch.cat(agg_layers_list, dim=0).to('cpu').numpy()\n",
        "    labels = np.concatenate(label_list, axis=0)\n",
        "    predictions = np.concatenate(pred_list, axis=0)\n",
        "\n",
        "    return agg_layers_list, labels, predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_distribution(train_dataloader,test_dataloader,ood_dataloader,num_layers): \n",
        "  #returns all the features extracted from BERT for IN-data and OOD-data\n",
        "  train_features = extract_bert_features(train_dataloader,num_layers)\n",
        "  test_features = extract_bert_features(test_dataloader,num_layers)\n",
        "  ood_features = extract_bert_features(ood_dataloader,num_layers)\n",
        "\n",
        "  train_mean = np.mean(train_features[0], axis=0) \n",
        "               \n",
        "  train_cov = np.cov(train_features[0], rowvar=False)\n",
        "                                          \n",
        "  train_inv_cov = np.linalg.inv(train_cov)\n",
        "\n",
        "  return train_mean,train_cov,train_inv_cov,test_features,ood_features\n",
        "\n"
      ],
      "metadata": {
        "id": "o82OHfDbjDC4"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def results(train_dataloader,test_dataloader,ood_dataloader,num_layers):\n",
        "\n",
        "  a = train_distribution(train_dataloader,test_dataloader,ood_dataloader,num_layers)\n",
        "  \n",
        "  train_mean = a[0]\n",
        "  train_cov = a[1]\n",
        "  train_inv_cov = a[2]\n",
        "  test_features = a[3]\n",
        "  ood_features = a[4]\n",
        "\n",
        "  inds_test_scores = []\n",
        "  ood_test_scores = []\n",
        "\n",
        "  for feature in test_features[0]:\n",
        "     score = mahalanobis(feature, train_mean, train_inv_cov)\n",
        "     inds_test_scores.append(score)\n",
        "\n",
        "  for feature in ood_features[0]:\n",
        "    score = mahalanobis(feature, train_mean, train_inv_cov)\n",
        "    ood_test_scores.append(score)\n",
        "\n",
        "  \n",
        "  labels = np.concatenate([np.zeros(len(inds_test_scores)), np.ones(len(ood_test_scores))])\n",
        "  scores = np.concatenate([inds_test_scores, ood_test_scores])\n",
        "\n",
        "  threshold = np.mean(inds_test_scores) +  np.std(inds_test_scores)\n",
        "\n",
        "  FPR, ERR = metrics (scores, labels, threshold)\n",
        "  auroc = roc_auc_score(labels, scores)\n",
        "  aupr = average_precision_score(labels, scores)\n",
        "\n",
        "  return auroc,aupr,ERR\n",
        "\n"
      ],
      "metadata": {
        "id": "r9vuSQankNzG"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUROC = []\n",
        "AUPR = []\n",
        "ERR = []\n",
        "\n",
        "for num_layers in range(2,15):\n",
        "  auroc,aupr,err = results(train_dataloader,test_dataloader,ood_dataloader,num_layers)\n",
        "  AUROC.append(auroc)\n",
        "  AUPR.append(aupr)\n",
        "  ERR.append(err)\n",
        "  print(str(num_layers))\n",
        "  print('AUROC:', auroc)\n",
        "  print('AUPR:', aupr)\n",
        "  print('ERR:', ERR)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FKYJwkVglrjd",
        "outputId": "b74550d8-a5df-468e-dfbe-5da248124e89"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "AUROC: 0.7712371066289995\n",
            "AUPR: 0.7311687097273454\n",
            "ERR: [0.36991779604532327]\n",
            "3\n",
            "AUROC: 0.7845787219809951\n",
            "AUPR: 0.7452470359975572\n",
            "ERR: [0.36991779604532327, 0.35525438791379693]\n",
            "4\n",
            "AUROC: 0.7929067297374242\n",
            "AUPR: 0.7518104322184359\n",
            "ERR: [0.36991779604532327, 0.35525438791379693, 0.34770051099755606]\n",
            "5\n",
            "AUROC: 0.7922272458505295\n",
            "AUPR: 0.7527720070035466\n",
            "ERR: [0.36991779604532327, 0.35525438791379693, 0.34770051099755606, 0.3479226838480338]\n",
            "6\n",
            "AUROC: 0.7807017019946525\n",
            "AUPR: 0.7428963578289083\n",
            "ERR: [0.36991779604532327, 0.35525438791379693, 0.34770051099755606, 0.3479226838480338, 0.3628082648300378]\n",
            "7\n",
            "AUROC: 0.7737856191505754\n",
            "AUPR: 0.7395086995182332\n",
            "ERR: [0.36991779604532327, 0.35525438791379693, 0.34770051099755606, 0.3479226838480338, 0.3628082648300378, 0.3696956231948456]\n",
            "8\n",
            "AUROC: 0.7521615869621915\n",
            "AUPR: 0.7156088034477636\n",
            "ERR: [0.36991779604532327, 0.35525438791379693, 0.34770051099755606, 0.3479226838480338, 0.3628082648300378, 0.3696956231948456, 0.39857809375694286]\n",
            "9\n",
            "AUROC: 0.7372527588479233\n",
            "AUPR: 0.6995386488525618\n",
            "ERR: [0.36991779604532327, 0.35525438791379693, 0.34770051099755606, 0.3479226838480338, 0.3628082648300378, 0.3696956231948456, 0.39857809375694286, 0.41257498333703624]\n",
            "10\n",
            "AUROC: 0.7350815161668587\n",
            "AUPR: 0.6976585965281104\n",
            "ERR: [0.36991779604532327, 0.35525438791379693, 0.34770051099755606, 0.3479226838480338, 0.3628082648300378, 0.3696956231948456, 0.39857809375694286, 0.41257498333703624, 0.4156854032437236]\n",
            "11\n",
            "AUROC: 0.7454179134489232\n",
            "AUPR: 0.7088164635384269\n",
            "ERR: [0.36991779604532327, 0.35525438791379693, 0.34770051099755606, 0.3479226838480338, 0.3628082648300378, 0.3696956231948456, 0.39857809375694286, 0.41257498333703624, 0.4156854032437236, 0.4059097978227061]\n",
            "12\n",
            "AUROC: 0.7577095440310735\n",
            "AUPR: 0.7227513086897932\n",
            "ERR: [0.36991779604532327, 0.35525438791379693, 0.34770051099755606, 0.3479226838480338, 0.3628082648300378, 0.3696956231948456, 0.39857809375694286, 0.41257498333703624, 0.4156854032437236, 0.4059097978227061, 0.3934681181959565]\n",
            "13\n",
            "AUROC: 0.7586707360046825\n",
            "AUPR: 0.7224000684120431\n",
            "ERR: [0.36991779604532327, 0.35525438791379693, 0.34770051099755606, 0.3479226838480338, 0.3628082648300378, 0.3696956231948456, 0.39857809375694286, 0.41257498333703624, 0.4156854032437236, 0.4059097978227061, 0.3934681181959565, 0.39035769828926903]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-464f317e7bc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mauroc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maupr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mood_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mAUROC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauroc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mAUPR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maupr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-618c3a02d84f>\u001b[0m in \u001b[0;36mresults\u001b[0;34m(train_dataloader, test_dataloader, ood_dataloader, num_layers)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mood_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mood_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtrain_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtrain_cov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtrain_inv_cov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-ee0e67663079>\u001b[0m in \u001b[0;36mtrain_distribution\u001b[0;34m(train_dataloader, test_dataloader, ood_dataloader, num_layers)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mood_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_bert_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_bert_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mood_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_bert_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mood_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-155fe716103f>\u001b[0m in \u001b[0;36mextract_bert_features\u001b[0;34m(loader, num_layers)\u001b[0m\n\u001b[1;32m     12\u001b[0m             outputs = model(b_input_ids, token_type_ids=None,\n\u001b[1;32m     13\u001b[0m                             attention_mask=b_input_mask)\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0magg_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-a9b53dcda0f5>\u001b[0m in \u001b[0;36maggregation\u001b[0;34m(all_layers, num_layers)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0magg_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0magg_layers\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0magg_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHdNAnP6D_VB"
      },
      "source": [
        "FIN SAMI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkRz5Ima2Oeo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}